{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tunrham/UTS_ML2019_ID11403868/blob/master/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-NOZc_Y9pNQ",
        "colab_type": "text"
      },
      "source": [
        "# Report Review for \"_Superhuman AI for heads-up no-limit poker: Libratus beats top professionals_\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPaGG9SRRdzJ",
        "colab_type": "text"
      },
      "source": [
        "Assignment GitHub Repository: https://github.com/tunrham/UTS_ML2019_ID11403868/\n",
        "\n",
        "Assignment Name: 'A1'\n",
        "\n",
        "Assignment URL: https://github.com/Tunrham/UTS_ML2019_ID11403868/blob/master/A1.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qj94EHR8Ej3v"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWHPXBVvEsOk",
        "colab_type": "text"
      },
      "source": [
        "In the Machine Learning landscape, a culturally popularised theme is “Man vs Machines”. Well-documented examples include Gary Kasparoff losing to Deep Blue in Chess (1997), and Lee Sedol losing to AlphaGo in Go (2016). However, one area where humanity has consistently prevailed, has been in complex, large state, and imperfect-information based games – where the necessary machine learning methodology, and computational power didn’t exist. Two player Poker has often been heralded as the “benchmark challenge” for AI relative to these games. Noam Brown and Toumas Sandholm’s “*Superhuman AI for heads-up no-limit poker: Libratus beats top professionals*” (2017) reverses this narrative, documenting the methodology and execution of the first  AI (named “Libratus”) that was able to beat the best humans in a game of imperfect information: No Limit Texas Hold’em Poker. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAcHNoP49pNS",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HsTsayJdDjUz"
      },
      "source": [
        "Brown and Sandholm’s research paper is segmented into three sections; introducing the challenges solving imperfect-information games, the methodologies for finding solutions for poker, and the experimental evaluation validating such methodologies. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRYvuNBq9pNT",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgx5-N-4Fydl",
        "colab_type": "text"
      },
      "source": [
        "The paper’s introduction posits the difficulty solving games of imperfect-information given the number of unique (and hidden) game states. It explores the importance of such solutions due to their structure in everyday life, existing in business strategy, negotiations, and auctions. The paper outlines methodologies for estimating a best approximation Nash Equilibrium strategy - whereby neither player can improve their payoff in any game state through deviation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6JJSAieFzxV",
        "colab_type": "text"
      },
      "source": [
        "### Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykHQHMrJF8-s",
        "colab_type": "text"
      },
      "source": [
        "#### First Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drjC2PkUGPJ9",
        "colab_type": "text"
      },
      "source": [
        "The methodology section of the paper explores this using a tiered approach featuring three modules: A First Level Abstraction, Second Level Abstraction, and a Self-Improver. Libratus’ First Level Abstraction uses a Monte Carlo Counterfactual Regret Minimisation (MCCFR) Algorithm, creating a manageable blueprint abstraction of the entire game. MCCFR is an iterated process, aiming to converge the regret score of a decision (a score that a decision costs its decision-maker) as close to zero as possible. This finds Libratus an approximation Nash Equilibrium blueprint strategy suitable for early rounds of play. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hBvwCrcGRGU",
        "colab_type": "text"
      },
      "source": [
        "#### Second Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDdvQU9sGU5d",
        "colab_type": "text"
      },
      "source": [
        "Libratus’ Second Level Abstraction focuses on the final game rounds, or, any small-sized game state. Here Libratus creates a more defined abstraction within the blueprint strategy known as a subgame. Libratus solves the defined subgame abstraction whilst accounting for other unreached subgames - which may influence decision making. Using this method at inflexion points in the game tree instead of defaulting to the blueprint strategy allows for a more nuanced approach for subgame scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exCVChnFGWkl",
        "colab_type": "text"
      },
      "source": [
        "#### Third Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI_PKZbkGaBE",
        "colab_type": "text"
      },
      "source": [
        "Libratus’ last module is a Self-Improver, which explores it’s opponents most frequent in-game betting patterns in the first round of play outside of Libratus’ blueprint strategy. An equilibrium response is then calculated and added to the blueprint strategy, allowing for the creation of a specified response abstraction for commonly encountered actions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hl8m0U0Gljc",
        "colab_type": "text"
      },
      "source": [
        "### Experimental Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVEAN_mgG3KX",
        "colab_type": "text"
      },
      "source": [
        "The final section of the paper outlines the experimental evaluation undertaken to validate Libratus’ methodology. Initially, Libratus was tested using a simplified game tree, consisting of less game states.  After showing positive results, Libratus was tested against the strongest available AI, and finally, once adjusted, Libratus was tested against the best human competition in 2017. Here Libratus convincingly beat four top Professional Poker players over a statistically significant sample. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr28sH7k9pNU",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qexxzSfD9pNV",
        "colab_type": "text"
      },
      "source": [
        "The machine learning space in Poker was active before Libratus’ development – Brown and Sandholm’s previous AI “Claudico” was beaten in a similar experiment just two years prior. Existing AI such as “Tartanian7” (Brown, Ganzfried, and Sandholm, 2015) and “Baby Tartanian8” (Brown and Sandholm 2016) were already using MCCFR for blueprint strategy abstraction, and “DeepStack” (Moravcik et al. 2017) forewent this solution, using deep neural networks and beating some human players, albeit in a less significant experimental evaluation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVivNlN9HcjF",
        "colab_type": "text"
      },
      "source": [
        "### MCCFR Improvements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWhflLkDHpvs",
        "colab_type": "text"
      },
      "source": [
        "Each of Libratus’ three modules represent either an improvement or an innovation in the machine learning Poker space. Whilst MCCFR had been extensively used in similar Poker AI, Libratus’ first module uses an improved MCCFR function which avoids searching in-game actions that have high regret scores in future iterations.  This led to increased computational speed and space available, allowing solutions to larger abstractions that would have otherwise been impossible. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDvS-L-cHr3N",
        "colab_type": "text"
      },
      "source": [
        "### Nested Safe Subgame Solving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rw6h3NhHxh8",
        "colab_type": "text"
      },
      "source": [
        "Traditional Poker AI were static, and faced “Action Abstraction” problems, forcing sub-optimal reactions facing unmapped actions. This was due to using “Unsafe Subgame Solving”, which assumed the opposing player was following the AI’s blueprint strategy when finding solutions to subgames (Brown and Sandholm, 2017). Libratus’ second module; “Nested Safe Subgame Solving” combats this issue. This had Libratus follow the first module blueprint strategy until reaching a point where a more granular abstraction was required. A real-time counter action was then calculated, comparing all possible subgame responses with the proposed blueprint strategy and evaluating the differences between both to find an optimal response. Brown and Sandholm refer to this as “Estimated-Maxmargin”. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF5ZTC1zH0Pc",
        "colab_type": "text"
      },
      "source": [
        "### Self-Improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "05jdRKOEIERK"
      },
      "source": [
        "Libratus’ third “Self-Improvement” module represents the final distinguishing factor between its counterparts. Traditional machine learning models attempted to exploit imbalances in opposing strategies, however in doing so, used exploitable strategies themselves. Libratus tackled this problem by observing its opponent’s most frequent in-game bet sizes in the early rounds of play, and then redefining its baseline strategy by converging two unique self-improvement models to find the counter-response with the lowest regret score. This added a near unexploitable response to the blueprint abstraction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4UK_0pA9pNW",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvhIoHyQ9pNW",
        "colab_type": "text"
      },
      "source": [
        "### Technical Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRd8DAYJIOjN",
        "colab_type": "text"
      },
      "source": [
        "Brown and Sandholm’s paper outlines the context and challenges faced, and the solutions employed in the creation of Libratus. Furthermore, given little framework methodology for evaluating Poker AI - the experimental evaluation clearly displays Libratus’ advantage over competition. The paper also excels in explaining itself without a nuanced understanding of Poker. However, without explaining these nuances, the paper is non-replicable, and with such knowledge, requires further clarification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja9eEUeLIPc1",
        "colab_type": "text"
      },
      "source": [
        "### Notable Omissions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqO2dX1GITKN",
        "colab_type": "text"
      },
      "source": [
        "An example of an omitted nuance; is whilst testing against humans Brown and Sandholm used two techniques for variance minimisation - outlined in The Fight For Humanity Rages On! (Polk 2017, 13:30). The first; “Mirroring”, involves grouping the human team into pairs, and distributing identically opposing hands for each AI vs Human pairing for each paired game. The second; “Equity Chopping”, is where two all in hands with community cards to come split the pot based on the percentage each hand has of winning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVLQaJFRIJJu",
        "colab_type": "text"
      },
      "source": [
        "Finally, the paper barely expands on the testing methodology used against humans, which given the title, is disappointing. Another omitted example of this is how the human players were playing around 10 hours a day each (followed by studying gameplay together looking for weak points in Libratus) in order to reach the target number of hands – an aspect which they suggested didn’t change the final result, but did have a minimal impact due to burnout.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Ahc7Df9pNX",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXJ3Nf4k9pNY",
        "colab_type": "text"
      },
      "source": [
        "### Application Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_c0W5K4In8F",
        "colab_type": "text"
      },
      "source": [
        "Whilst difficult to evaluate theoretically, Brown and Sandholm clearly introduced strong methodologies and technical analysis for machine learning in Poker AI. As further vindication, Libratus beat the best existing competition over a statistically significant sample – an impressive measure of success. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYrFHTqGIu6l",
        "colab_type": "text"
      },
      "source": [
        "### Further Developments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J2AqXtgI8Sd",
        "colab_type": "text"
      },
      "source": [
        "The techniques Brown and Sandholm used are strong consolidation points for machine learning and evaluating large state imperfect-information scenarios. In the research area, a logical next step is transitioning to the more popular Six or Nine Player Poker variants. These are more computationally exhaustive versions of the game due to the increased imperfect-information, and certain co-operative non-intuitive equilibrium game states. Similarly, transposing the methodologies to “real-world” applications using research data such as market analysis and strategy represents another exciting usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5UvfmxfJFzk",
        "colab_type": "text"
      },
      "source": [
        "### Personal Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPvEhH_OJLec",
        "colab_type": "text"
      },
      "source": [
        "Finally, I believe that the paper is a fun and personable conversation piece regarding machine learning. It combines a game steeped in familiarity, with unique machine learning and analytics-based solutions - highlighting relevant developments in the industry alongside a marketable “Man vs Machine” narrative. As an avid poker player, the combination of a hobby with my field of study made the paper highly engaging. What I found most interesting was that Libratus is never exploiting opponents – it is always looking for an Equilibrium solution. This means that whilst Libratus beat the top Human Professionals, they can beat weaker competition at a higher win rate than Libratus. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBgPeGCX9pNZ",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hhoAbNr9pNZ",
        "colab_type": "text"
      },
      "source": [
        "Brown and Sandholm’s paper is professional with its deliverance, featuring a logical flow from the introduction of the problem (including the context), to their unique solutions, and, the experimental method of evaluation. The paper also appropriately uses diagrams, further expounding on its methodology. It is impressive that the paper is comprehensible without delving into nuanced Poker terminology. However, one small confusing area (as a poker player) was using milli-Big Blinds per game (mbb/hand) as the scoring metric. The widely accepted method in the online poker community is Big Blinds per 100 Hands (BB/100), however, to appease both poker playing and non-poker playing audiences, the paper could have used dollars per 100 hands ($/100) for simplification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zoE431f9pNa",
        "colab_type": "text"
      },
      "source": [
        "## References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8cKnZOSLqls",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "[1][1]: Brown, N., Ganzfried, S., Sandholm, T., 2015, ‘Tartanian7: A Champion Two-Player No-Limit Texas Hold’em Poker-Playing Program’, AAAI 2015\n",
        "\n",
        "[2][2]: Brown, N., Sandholm, T., 2016, ‘Baby Tartanian8: Winning Agent from the 2016 Annual Computer Poker Competition’, *IJCAI’16 Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence*, pp 4238-4239\n",
        "\n",
        "[3][3]: Moravcik, M., Schmid, M., Burch, N., Lisy, V., Morrill, D., Bard, N., Davis, T., Waugh, K., Johanson, M., Bowling, M., 2017, ‘DeepStack: Expert-Level Artificial Interlligence in Heads-Up No-Limit Poker’, AAAS Science, Vol.356, issue 6337, pp 508-513\n",
        "\n",
        "[4][4]: Brown, N., Sandholm, T., 2017, ‘Safe and Nested Subgame Solving for Imperfect-Information Games’, Sourced <https://www.cs.cmu.edu/~noamb/papers/17-arXiv-Subgame.pdf> at 18/08/2019\n",
        "\n",
        "[5][5]:Polk, D., 2017, *The Fight For Humanity Rages On!*, video recording, YouTube, viewed 19/08/2019, <https://youtu.be/crgmYTMfrSc> \n",
        "\n",
        "[1]:https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/viewFile/9962/9845\n",
        "\n",
        "[2]:https://www.cs.cmu.edu/~sandholm/BabyTartanian8.ijcai16demo.pdf\n",
        "\n",
        "[3]:https://arxiv.org/pdf/1701.01724.pdf\n",
        "\n",
        "[4]:https://www.cs.cmu.edu/~noamb/papers/17-arXiv-Subgame.pdf\n",
        "\n",
        "[5]:https://youtu.be/crgmYTMfrSc"
      ]
    }
  ]
}